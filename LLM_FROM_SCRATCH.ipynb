{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer\n",
    "## DataLoader\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from pathlib import Path\n",
    "import tiktoken\n",
    "DATASET_PATH  = Path(\"./datasets\")\n",
    "sample_chat_path = DATASET_PATH/\"chat.txt\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants\n",
    "CONTEXT_LENGTH=256\n",
    "EMBEDDING_DIMENSION=256\n",
    "POSITION_EMEBEDDING_DIMENSION=256\n",
    "BATCH_SIZE=4\n",
    "STRIDE=256\n",
    "VOCAB_SIZE=tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sampling with a  sliding window\n",
    "- Way to create an input-target pairs for the next-word prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample_chat_path,\"r\",encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "encoded_text = tokenizer.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChatDataSet(Dataset):\n",
    "    def __init__(self,txt,tokenizer,context_length,stride):\n",
    "        # tokenizer is the tokenizer object\n",
    "        self.tokenizer = tokenizer\n",
    "        # input ids and output ids are the length of tensors where each row is a tokenized text of length of context length\n",
    "        self.input_ids = []\n",
    "        self.output_ids =[]\n",
    "        token_ids = self.tokenizer.encode(txt)\n",
    "        token_ids_length = len(token_ids)\n",
    "        # Creation of dataset each row contains the tokenized text of the context length\n",
    "        # Stride represents the number of tokens to skip\n",
    "        for i in range(0,token_ids_length-context_length,stride):\n",
    "            # Here we are generating all the possible input pair and output pair\n",
    "            input_chunk = token_ids[i:i+context_length]\n",
    "            output_chunk  = token_ids[i+1:i+context_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.output_ids.append(torch.tensor(output_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self,index):\n",
    "        return self.input_ids[index],self.output_ids[index]\n",
    "    \n",
    "## DataLoader\n",
    "\n",
    "def create_dataloader_v1(txt:str,batch_size=4,context_length=256,stride=256,shuffle = True):\n",
    "    dataset = ChatDataSet(txt,tokenizer,context_length,stride)\n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = create_dataloader_v1(text,batch_size = BATCH_SIZE,context_length = CONTEXT_LENGTH,stride=STRIDE,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial token embeddings\n",
    "\n",
    "- output_dim = 256\n",
    "- vocab_size = 50257\n",
    "- shape of the embedding_layer = (vocab_size,output_dim) = (50257,256)\n",
    "- shape of the positional embedding layer = (context_length,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding_layer = torch.nn.Embedding(VOCAB_SIZE,EMBEDDING_DIMENSION)\n",
    "positional_embedding_layer = torch.nn.Embedding(CONTEXT_LENGTH,POSITION_EMEBEDDING_DIMENSION)\n",
    "pos_embeddings = positional_embedding_layer(torch.arange(CONTEXT_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample example of data passing\n",
    "data_iter = iter(data_loader)\n",
    "first_batch  = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = token_embedding_layer(input)\n",
    "input_to_LLM = token_embeddings+pos_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 256])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_to_LLM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter - 3 (Coding attention Mechanisms)\n",
    "\n",
    "- Exploring the reasons for using attention mechanisms in neural networks\n",
    "- Masking randomly selected attention weights with dropout to reduce over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[0.43,0.15,0.89], # Your \n",
    "                       [0.55,0.87,0.66], # journey\n",
    "                       [0.57,0.85,0.64], # starts\n",
    "                       [0.22,0.58,0.33], # with\n",
    "                       [0.77,0.25,0.10], # one\n",
    "                       [0.05,0.80,0.55]])# step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the dot poduct ( Essentially checking how two embeddings are similar)\n",
    "attention_weights = inputs@inputs.T\n",
    "# Normalize the score with softmax\n",
    "normalized_weights_with_softmax  = torch.softmax(attention_weights,dim=1)\n",
    "# Get the weighted representation of the token with relation to the other token\n",
    "context_vector = normalized_weights_with_softmax@inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the end of simple attention mechanisms\n",
    "\n",
    "### Start of computing the attention weights step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A simple step by step implementaion of the weighted attention mechanism\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "w_query = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=True)\n",
    "w_key = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=True)\n",
    "w_value = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=True)\n",
    "queries = inputs@w_query\n",
    "keys = inputs@w_key\n",
    "values = inputs@w_value\n",
    "unnormalized_attention_score =queries@keys.T\n",
    "normalized_attention_score = torch.softmax(unnormalized_attention_score/d_out**0.5,dim=1)\n",
    "context_vector = normalized_attention_score@values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a compact self-attention Python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self,d_in,d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.w_query = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.w_key = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.w_value = nn.Parameter(torch.rand(d_in,d_out))\n",
    "    def forward(self,x):\n",
    "        queries  = x@self.w_query\n",
    "        keys = x@self.w_key\n",
    "        values = x@self.w_value\n",
    "        # Calcualte the scores \n",
    "        attn_scores = queries@keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/(self.d_out**0.5),dim =-1)\n",
    "        return attn_weights@values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,attn_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.w_query = nn.Linear(d_in,d_out,bias=attn_bias)\n",
    "        self.w_key = nn.Linear(d_in,d_out,bias=attn_bias)\n",
    "        self.w_value = nn.Linear(d_in,d_out,bias=attn_bias)\n",
    "    def forward(self,x):\n",
    "        context_length = x.shape[0]\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "        #Calculate the scores\n",
    "        attn_scores = queries@keys.T\n",
    "        #  Masking the weights only tells LLM to refer the previous context to generate the next token\n",
    "        mask = torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "        masked_attn_scores = attn_scores.masked_fill(mask==1,float(\"-inf\"))\n",
    "        attn_weights = torch.softmax(masked_attn_scores/(self.d_out**0.5),dim=-1)\n",
    "        print(attn_weights)\n",
    "        return attn_weights@values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the casual attention\n",
    "\n",
    "Here we will make the weights of the forward matrix to zero to tell the llm only see the previous tokens to generate the current token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4400, 0.5600, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2830, 0.3580, 0.3590, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2264, 0.2579, 0.2583, 0.2574, 0.0000, 0.0000],\n",
      "        [0.1903, 0.2024, 0.2026, 0.1997, 0.2051, 0.0000],\n",
      "        [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4772, 0.1063],\n",
      "        [0.5891, 0.3257],\n",
      "        [0.6202, 0.3860],\n",
      "        [0.5478, 0.3589],\n",
      "        [0.5321, 0.3428],\n",
      "        [0.5077, 0.3493]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ca = CasualAttention(3,2)\n",
    "print(ca(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking additional attention weights with dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a compact casual attention class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttentionWithBatch(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,qkv_bias):\n",
    "        super().__init__()\n",
    "        # d_out represents the total output dimension , each head contributes to subset of dimension\n",
    "        self.d_out = d_out\n",
    "        # This is the query layer we were using having the dimension (input dimension is the same as the embedding dimension of the input)\n",
    "        self.w_query = nn.Linear(d_in,d_out,qkv_bias)\n",
    "        # This is the key layer we were using having the dimension (input dimension is the same as the embeding dimension of the input)\n",
    "        self.w_key = nn.Linear(d_in,d_out,qkv_bias)\n",
    "        # This is the value layer we were using having the dimension (input dimension is the same as the embedding dimension of the input)\n",
    "        self.w_value = nn.Linear(d_in,d_out,qkv_bias)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "    def forward(self,x):\n",
    "        keys = self.w_key(x)\n",
    "        queries = self.w_query(x)\n",
    "        values = self.w_value(x)\n",
    "        attention_scores = keys@queries.transpose(1,2)\n",
    "        attention_scores.masked_fill(self.mask==1,-torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores/(self.d_out**0.5),dim = 2)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        return attention_weights@values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab = CasualAttentionWithBatch(3,2,4,0.5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1024x256 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cab(input_to_LLM)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 15\u001b[0m, in \u001b[0;36mCasualAttentionWithBatch.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 15\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_key(x)\n\u001b[1;32m     16\u001b[0m     queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_query(x)\n\u001b[1;32m     17\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_value(x)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1024x256 and 3x2)"
     ]
    }
   ],
   "source": [
    "cab(input_to_LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Single-head attention to multi-head attention\n",
    "## Stacking multiple single-head attention layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_heads,kqv_bias):\n",
    "        super().__init__()\n",
    "        assert d_out%num_heads==0,\"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.w_key = nn.Linear(d_in,d_out,bias=kqv_bias)\n",
    "        self.w_query = nn.Linear(d_in,d_out,bias=kqv_bias)\n",
    "        self.w_value = nn.Linear(d_in,d_out,bias=kqv_bias)\n",
    "        self.head_dim = d_out//num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.out_proj = nn.Linear(d_out,d_out)\n",
    "    def forward(self,x):\n",
    "        batch_size,context_length,_ = x.shape\n",
    "        keys = self.w_key(x)\n",
    "        queries = self.w_query(x)\n",
    "        values = self.w_value(x)\n",
    "        # Now for each weight tensor we need to reshape to (batch,context_length,num_heads,head_dim)\n",
    "        keys = keys.reshape(batch_size,context_length,self.num_heads,self.head_dim)\n",
    "        queries = queries.reshape(batch_size,context_length,self.num_heads,self.head_dim)\n",
    "        values = values.reshape(batch_size,context_length,self.num_heads,self.head_dim)\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        #(batch_size,self.num_heads,self.context_length,self.head_dim)\n",
    "        scores = keys@(queries.transpose(2,3))\n",
    "        scores.masked_fill(self.mask==1,-torch.inf)\n",
    "        attn_weights = torch.softmax(scores/(keys.shape[-1]**0.5),dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vector = (attn_weights@values).transpose(1,2).reshape(batch_size,context_length,self.d_out)\n",
    "        return self.out_proj(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(3,2,6,0.5,2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mha(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 Finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 starts (Implmeting  a Gpt model from scratch to generate text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\":tokenizer.n_vocab, # vocabulary of the tokenizer\n",
    "\"context_length\":1024 , # Context length GPT has access to\n",
    "\"emb_dim\":768, # Each token has the embedding dimension of 768 , The absolute postional embedding of the dimension will also be 768\n",
    "\"n_heads\":12, # Each \n",
    "\n",
    "\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
