{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer\n",
    "## DataLoader\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from pathlib import Path\n",
    "import tiktoken\n",
    "DATASET_PATH  = Path(\"./datasets\")\n",
    "sample_chat_path = DATASET_PATH/\"chat.txt\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants\n",
    "CONTEXT_LENGTH=256\n",
    "EMBEDDING_DIMENSION=256\n",
    "POSITION_EMEBEDDING_DIMENSION=256\n",
    "BATCH_SIZE=4\n",
    "STRIDE=256\n",
    "VOCAB_SIZE=tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sampling with a  sliding window\n",
    "- Way to create an input-target pairs for the next-word prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample_chat_path,\"r\",encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "encoded_text = tokenizer.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChatDataSet(Dataset):\n",
    "    def __init__(self,txt,tokenizer,context_length,stride):\n",
    "        # tokenizer is the tokenizer object\n",
    "        self.tokenizer = tokenizer\n",
    "        # input ids and output ids are the length of tensors where each row is a tokenized text of length of context length\n",
    "        self.input_ids = []\n",
    "        self.output_ids =[]\n",
    "        token_ids = self.tokenizer.encode(txt)\n",
    "        token_ids_length = len(token_ids)\n",
    "        # Creation of dataset each row contains the tokenized text of the context length\n",
    "        # Stride represents the number of tokens to skip\n",
    "        for i in range(0,token_ids_length-context_length,stride):\n",
    "            # Here we are generating all the possible input pair and output pair\n",
    "            input_chunk = token_ids[i:i+context_length]\n",
    "            output_chunk  = token_ids[i+1:i+context_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.output_ids.append(torch.tensor(output_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self,index):\n",
    "        return self.input_ids[index],self.output_ids[index]\n",
    "    \n",
    "## DataLoader\n",
    "\n",
    "def create_dataloader_v1(txt:str,batch_size=4,context_length=256,stride=256,shuffle = True):\n",
    "    dataset = ChatDataSet(txt,tokenizer,context_length,stride)\n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = create_dataloader_v1(text,batch_size = BATCH_SIZE,context_length = CONTEXT_LENGTH,stride=STRIDE,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial token embeddings\n",
    "\n",
    "- output_dim = 256\n",
    "- vocab_size = 50257\n",
    "- shape of the embedding_layer = (vocab_size,output_dim) = (50257,256)\n",
    "- shape of the positional embedding layer = (context_length,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding_layer = torch.nn.Embedding(VOCAB_SIZE,EMBEDDING_DIMENSION)\n",
    "positional_embedding_layer = torch.nn.Embedding(CONTEXT_LENGTH,POSITION_EMEBEDDING_DIMENSION)\n",
    "pos_embeddings = positional_embedding_layer(torch.arange(CONTEXT_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample example of data passing\n",
    "data_iter = iter(data_loader)\n",
    "first_batch  = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = token_embedding_layer(input)\n",
    "input_to_LLM = token_embeddings+pos_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 256])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_to_LLM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter - 3 (Coding attention Mechanisms)\n",
    "\n",
    "- Exploring the reasons for using attention mechanisms in neural networks\n",
    "- Masking randomly selected attention weights with dropout to reduce over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[0.43,0.15,0.89], # Your \n",
    "                       [0.55,0.87,0.66], # journey\n",
    "                       [0.57,0.85,0.64], # starts\n",
    "                       [0.22,0.58,0.33], # with\n",
    "                       [0.77,0.25,0.10], # one\n",
    "                       [0.05,0.80,0.55]])# step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the dot poduct ( Essentially checking how two embeddings are similar)\n",
    "attention_weights = inputs@inputs.T\n",
    "# Normalize the score with softmax\n",
    "normalized_weights_with_softmax  = torch.softmax(attention_weights,dim=1)\n",
    "# Get the weighted representation of the token with relation to the other token\n",
    "context_vector = normalized_weights_with_softmax@inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the end of simple attention mechanisms\n",
    "\n",
    "### Start of computing the attention weights step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A simple step by step implementaion of the weighted attention mechanism\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "w_query = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=True)\n",
    "w_key = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=True)\n",
    "w_value = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=True)\n",
    "queries = inputs@w_query\n",
    "keys = inputs@w_key\n",
    "values = inputs@w_value\n",
    "unnormalized_attention_score =queries@keys.T\n",
    "normalized_attention_score = torch.softmax(unnormalized_attention_score/d_out**0.5,dim=1)\n",
    "context_vector = normalized_attention_score@values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a compact self-attention Python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self,d_in,d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.w_query = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.w_key = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.w_value = nn.Parameter(torch.rand(d_in,d_out))\n",
    "    def forward(self,x):\n",
    "        queries  = x@self.w_query\n",
    "        keys = x@self.w_key\n",
    "        values = x@self.w_value\n",
    "        # Calcualte the scores \n",
    "        attn_scores = queries@keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/(self.d_out**0.5),dim =-1)\n",
    "        return attn_weights@values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,attn_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.w_query = nn.Linear(d_in,d_out,bias=attn_bias)\n",
    "        self.w_key = nn.Linear(d_in,d_out,bias=attn_bias)\n",
    "        self.w_value = nn.Linear(d_in,d_out,bias=attn_bias)\n",
    "    def forward(self,x):\n",
    "        context_length = x.shape[0]\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "        #Calculate the scores\n",
    "        attn_scores = queries@keys.T\n",
    "        #  Masking the weights only tells LLM to refer the previous context to generate the next token\n",
    "        mask = torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "        masked_attn_scores = attn_scores.masked_fill(mask==1,float(\"-inf\"))\n",
    "        attn_weights = torch.softmax(masked_attn_scores/(self.d_out**0.5),dim=-1)\n",
    "        print(attn_weights)\n",
    "        return attn_weights@values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the casual attention\n",
    "\n",
    "Here we will make the weights of the forward matrix to zero to tell the llm only see the previous tokens to generate the current token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4400, 0.5600, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2830, 0.3580, 0.3590, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2264, 0.2579, 0.2583, 0.2574, 0.0000, 0.0000],\n",
      "        [0.1903, 0.2024, 0.2026, 0.1997, 0.2051, 0.0000],\n",
      "        [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4772, 0.1063],\n",
      "        [0.5891, 0.3257],\n",
      "        [0.6202, 0.3860],\n",
      "        [0.5478, 0.3589],\n",
      "        [0.5321, 0.3428],\n",
      "        [0.5077, 0.3493]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ca = CasualAttention(3,2)\n",
    "print(ca(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking additional attention weights with dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a compact casual attention class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttentionWithBatch(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,qkv_bias):\n",
    "        super().__init__()\n",
    "        # d_out represents the total output dimension , each head contributes to subset of dimension\n",
    "        self.d_out = d_out\n",
    "        # This is the query layer we were using having the dimension (input dimension is the same as the embedding dimension of the input)\n",
    "        self.w_query = nn.Linear(d_in,d_out,qkv_bias)\n",
    "        # This is the key layer we were using having the dimension (input dimension is the same as the embeding dimension of the input)\n",
    "        self.w_key = nn.Linear(d_in,d_out,qkv_bias)\n",
    "        # This is the value layer we were using having the dimension (input dimension is the same as the embedding dimension of the input)\n",
    "        self.w_value = nn.Linear(d_in,d_out,qkv_bias)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "    def forward(self,x):\n",
    "        keys = self.w_key(x)\n",
    "        queries = self.w_query(x)\n",
    "        values = self.w_value(x)\n",
    "        attention_scores = keys@queries.transpose(1,2)\n",
    "        attention_scores.masked_fill(self.mask==1,-torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores/(self.d_out**0.5),dim = 2)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        return attention_weights@values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab = CasualAttentionWithBatch(3,2,4,0.5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Single-head attention to multi-head attention\n",
    "## Stacking multiple single-head attention layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_heads,kqv_bias):\n",
    "        super().__init__()\n",
    "        assert d_out%num_heads==0,\"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.w_key = nn.Linear(d_in,d_out,bias=kqv_bias)\n",
    "        self.w_query = nn.Linear(d_in,d_out,bias=kqv_bias)\n",
    "        self.w_value = nn.Linear(d_in,d_out,bias=kqv_bias)\n",
    "        self.head_dim = d_out//num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.out_proj = nn.Linear(d_out,d_out)\n",
    "    def forward(self,x):\n",
    "        batch_size,context_length,_ = x.shape\n",
    "        keys = self.w_key(x)\n",
    "        queries = self.w_query(x)\n",
    "        values = self.w_value(x)\n",
    "        # Now for each weight tensor we need to reshape to (batch,context_length,num_heads,head_dim)\n",
    "        keys = keys.reshape(batch_size,context_length,self.num_heads,self.head_dim)\n",
    "        queries = queries.reshape(batch_size,context_length,self.num_heads,self.head_dim)\n",
    "        values = values.reshape(batch_size,context_length,self.num_heads,self.head_dim)\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        #(batch_size,self.num_heads,self.context_length,self.head_dim)\n",
    "        scores = keys@(queries.transpose(2,3))\n",
    "        scores.masked_fill(self.mask==1,-torch.inf)\n",
    "        attn_weights = torch.softmax(scores/(keys.shape[-1]**0.5),dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vector = (attn_weights@values).transpose(1,2).reshape(batch_size,context_length,self.d_out)\n",
    "        return self.out_proj(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(3,2,6,0.5,2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mha(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 Finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 starts (Implmeting  a Gpt model from scratch to generate text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\":tokenizer.n_vocab, # vocabulary of the tokenizer\n",
    "\"context_length\":1024 , # Context length GPT has access to\n",
    "\"emb_dim\":768, # Each token has the embedding dimension of 768 , The absolute postional embedding of the dimension will also be 768\n",
    "\"n_heads\":12, # Each layer has individual 12 heads to learn the representations\n",
    "\"n_layers\":12, # Number of layers , I feel the repeating layers\n",
    "\"drop_rate\":0.1,# Drop out percentage\n",
    "\"kqv_bias\":False, # Bool flag to add bias in each of the kqv weight matrix\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get an high level overview of the how all the things fit together\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"]) #This tells that we have an embedding dimension of the row of the number of vocab and columns of embed dimension\n",
    "        self.pos_embedding = nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"]) #Pos embedding of size (context_length,emb_dimension)\n",
    "        self.drop_embedding = nn.Embedding(cfg[\"drop_rate\"])\n",
    "        # This is the all the transformer layer to be calculated\n",
    "        self.transfom_block = nn.Sequential(*[DummyTransformLayer()])\n",
    "        # A final Layer norm\n",
    "        # There should be an output head ( Mapping from the dimension of embedding to the vocab size)\n",
    "    \n",
    "    def forward(self,idx):\n",
    "        batch_size,context_length = idx.shape\n",
    "        token_emb = self.token_embedding(idx)\n",
    "        pos_embeddings = self.pos_embedding(torch.arange(context_length))\n",
    "        embedding_input_to_llm = token_emb+pos_embeddings\n",
    "        embedding_input_to_llm = self.drop_embedding(embedding_input_to_llm)\n",
    "        # Input to the transform block\n",
    "        x = self.transfom_block(embedding_input_to_llm)\n",
    "        # Put out put to the final layer\n",
    "        \n",
    "        # Get the output logits\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale  = torch.nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = torch.nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1,keep_dim=True)\n",
    "        var = x.var(dim=-1,keep_dim=True)\n",
    "        norm_x = (x-mean)/torch.sqrt(var+self.eps) # Eps is added not to create a zero division error\n",
    "        return self.scale*norm_x+self.shift\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenzier = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text1 = \"Every effort makes you\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.append(tokenizer.encode(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.append(tokenizer.encode(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6109, 3626, 1838, 345], [6109, 1110]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_arrays = torch.tensor([0.22,0.34,0.00,0.22,0.00,0.00])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_examples = torch.randn(2,5)\n",
    "layer = torch.nn.Sequential(torch.nn.Linear(5,6),torch.nn.ReLU())\n",
    "out  = layer(batch_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = out.mean(dim=-1,keepdim=True)\n",
    "var = out.var(dim=-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_norm =  (out-mean)/torch.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000]], grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_norm.var(dim=1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
